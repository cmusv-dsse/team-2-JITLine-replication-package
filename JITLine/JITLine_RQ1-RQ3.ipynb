{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "from my_util import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, matthews_corrcoef, precision_recall_fscore_support, classification_report, auc\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import differential_evolution\n",
    "import pandas as pd\n",
    "import time, pickle, math, warnings, os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "projects = ['openstack','qt']\n",
    "sampling_methods = 'DE_SMOTE_min_df_3'\n",
    "\n",
    "remove_python_common_tokens = True\n",
    "\n",
    "create_path_if_not_exist('./data/')\n",
    "create_path_if_not_exist('./final_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_combined_df(code_commit, commit_id, label, metrics_df, count_vect):\n",
    "    code_df = pd.DataFrame()\n",
    "    code_df['commit_id'] = commit_id\n",
    "    code_df['code'] = code_commit\n",
    "    code_df['label'] = label\n",
    "    \n",
    "    code_df = code_df.sort_values(by='commit_id')\n",
    "    \n",
    "    metrics_df = metrics_df.sort_values(by='commit_id')\n",
    "    metrics_df = metrics_df.drop('commit_id',axis=1)\n",
    "    \n",
    "    code_change_arr = count_vect.transform(code_df['code']).astype(np.int16).toarray()\n",
    "    metrics_df_arr = metrics_df.to_numpy(dtype=np.float32)\n",
    "    \n",
    "    final_features = np.concatenate((code_change_arr,metrics_df_arr),axis=1)\n",
    "\n",
    "    return final_features, list(code_df['commit_id']), list(code_df['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_func(k, train_feature, train_label, valid_feature, valid_label):\n",
    "    smote = SMOTE(random_state=42, k_neighbors= int(np.round(k)), n_jobs=32)\n",
    "    train_feature_res, train_label_res = smote.fit_resample(train_feature, train_label)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "    clf.fit(train_feature_res, train_label_res)\n",
    "    \n",
    "    prob = clf.predict_proba(valid_feature)[:,1]\n",
    "    auc = roc_auc_score(valid_label, prob)\n",
    "    \n",
    "    return -auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below this cell is used to\n",
    "\n",
    "1. obtain the best k_neighbor of SMOTE (the value is rounded to int)\n",
    "2. resample train data using SMOTE with the best k_neighbor value\n",
    "3. train RF model and obtain prediction result from the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(cur_proj):\n",
    "    data_path = './data/'\n",
    "    model_path = './final_model/'\n",
    "        \n",
    "    train_code, train_commit, train_label = prepare_data(cur_proj, mode='train',\n",
    "                                                                  remove_python_common_tokens=remove_python_common_tokens)\n",
    "    test_code, test_commit, test_label = prepare_data(cur_proj, mode='test',\n",
    "                                                              remove_python_common_tokens=remove_python_common_tokens)\n",
    "\n",
    "    commit_metrics = load_change_metrics_df(cur_proj)\n",
    "    train_commit_metrics = commit_metrics[commit_metrics['commit_id'].isin(train_commit)]\n",
    "    test_commit_metrics = commit_metrics[commit_metrics['commit_id'].isin(test_commit)]\n",
    "    \n",
    "    count_vect = CountVectorizer(min_df=3, ngram_range=(1,1))\n",
    "    count_vect.fit(train_code)\n",
    "    \n",
    "    train_feature, train_commit_id, new_train_label = get_combined_df(train_code, train_commit, train_label, train_commit_metrics,count_vect)\n",
    "    test_feature, test_commit_id, new_test_label = get_combined_df(test_code, test_commit, test_label, test_commit_metrics,count_vect)\n",
    "\n",
    "    percent_80 = int(len(new_train_label)*0.8)\n",
    "    \n",
    "    final_train_feature = train_feature[:percent_80]\n",
    "    final_train_commit_id = train_commit_id[:percent_80]\n",
    "    final_new_train_label = new_train_label[:percent_80]\n",
    "    \n",
    "    valid_feature = train_feature[percent_80:]\n",
    "    valid_commit_id = train_commit_id[percent_80:]\n",
    "    valid_label = new_train_label[percent_80:]\n",
    "\n",
    "    print('load data of',cur_proj, 'finish')\n",
    "    \n",
    "    # bounds = [(1,20)]\n",
    "    # result = differential_evolution(objective_func, bounds, args=(final_train_feature, final_new_train_label, \n",
    "    #                                                               valid_feature, valid_label),\n",
    "    #                                popsize=10, mutation=0.7, recombination=0.3,seed=0)\n",
    "    \n",
    "    # smote = SMOTE(random_state=42, n_jobs=32, k_neighbors=int(np.round(result.x)))\n",
    "    # train_feature_res, train_label_res = smote.fit_resample(final_train_feature, final_new_train_label)\n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    train_feature_res, train_label_res = ros.fit_resample(train_feature, new_train_label)\n",
    "\n",
    "    # mlp_parameter_space = {\n",
    "    #     'hidden_layer_sizes': [(256,64,32), (64,32), (256, 32)],\n",
    "    #     'activation': ['tanh', 'relu'],\n",
    "    #     'solver': ['sgd', 'adam'],\n",
    "    #     'alpha': [0.0001, 0.05],\n",
    "    #     'learning_rate': ['constant','adaptive'],\n",
    "    # }\n",
    "    clf_name = 'RF'\n",
    "    clf_rf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1, verbose=True)\n",
    "    clf_svm = SVC(C=0.1, gamma=0.1, probability=True, verbose=True)\n",
    "    clf_log_reg = LogisticRegression(verbose=True)\n",
    "    \n",
    "    clf = clf_rf\n",
    "    # clf_mlp = MLPClassifier(hidden_layer_sizes=(50,50,50), max_iter=1000, activation='relu')\n",
    "    # clf_mlp_gs = GridSearchCV(clf_mlp, mlp_parameter_space, n_jobs=-1, cv=3, verbose=10)\n",
    "    print(\"training first model\")\n",
    "    trained_clf_rf, _ = train_eval_model(clf_rf, train_feature_res, train_label_res, \n",
    "                                       test_feature, new_test_label)\n",
    "    print(\"finished training the first model\")\n",
    "    trained_clf_svm, _ = train_eval_model(clf_svm, train_feature_res, train_label_res, \n",
    "                                       test_feature, new_test_label)\n",
    "    print(\"finished training the second model\")\n",
    "    trained_clf_log_reg, _ = train_eval_model(clf_log_reg, train_feature_res, train_label_res, \n",
    "                                       test_feature, new_test_label)\n",
    "    print(\"finished training the third model\")\n",
    "\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    estimators=[('rf', trained_clf_rf), ('svm', trained_clf_svm), ('log_reg', trained_clf_log_reg)]\n",
    "    clf_ensemble = VotingClassifier(estimators, voting='hard')\n",
    "\n",
    "    trained_clf, pred_df = train_eval_model(clf_ensemble, train_feature_res, train_label_res, \n",
    "                                       test_feature, new_test_label)\n",
    "    print(\"finished training model\")\n",
    "    pred_df['test_commit'] = test_commit_id\n",
    "    pred_df.to_csv(data_path+cur_proj+'_'+clf_name+'_'+sampling_methods+'_prediction_result.csv')\n",
    "\n",
    "    model_path = model_path+cur_proj+'_'+clf_name+'_'+sampling_methods+'.pkl'\n",
    "    pickle.dump(trained_clf, open(model_path, 'wb'))\n",
    "\n",
    "    print('finished',cur_proj)\n",
    "    print('-'*100)\n",
    "\n",
    "    # k_of_smote = result.x\n",
    "    # best_AUC_of_obj_func = result.fun\n",
    "    \n",
    "    return # k_of_smote, best_AUC_of_obj_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11973/11973 [00:23<00:00, 508.65it/s]\n",
      "100%|██████████| 1331/1331 [00:02<00:00, 608.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data of openstack finish\n",
      "training first model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   38.3s\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  1.0min finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 300 out of 300 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 184 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 300 out of 300 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training the first model\n",
      "[LibSVM]..........\n",
      "Warning: using -h 0 may be faster\n",
      "*\n",
      "optimization finished, #iter = 10509\n",
      "obj = -1627.170000, rho = -0.700000\n",
      "nSV = 21018, nBSV = 21018\n",
      "Total nSV = 21018\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "predict_proba is not available when  probability=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/chenglinwei/Documents/18668/team-2-JITLine-replication-package/JITLine/JITLine_RQ1-RQ3.ipynb Cell 6\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chenglinwei/Documents/18668/team-2-JITLine-replication-package/JITLine/JITLine_RQ1-RQ3.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m run_experiment(\u001b[39m'\u001b[39;49m\u001b[39mopenstack\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32m/Users/chenglinwei/Documents/18668/team-2-JITLine-replication-package/JITLine/JITLine_RQ1-RQ3.ipynb Cell 6\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(cur_proj)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenglinwei/Documents/18668/team-2-JITLine-replication-package/JITLine/JITLine_RQ1-RQ3.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m trained_clf_rf, _ \u001b[39m=\u001b[39m train_eval_model(clf_rf, train_feature_res, train_label_res, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenglinwei/Documents/18668/team-2-JITLine-replication-package/JITLine/JITLine_RQ1-RQ3.ipynb#W5sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m                                    test_feature, new_test_label)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenglinwei/Documents/18668/team-2-JITLine-replication-package/JITLine/JITLine_RQ1-RQ3.ipynb#W5sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfinished training the first model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chenglinwei/Documents/18668/team-2-JITLine-replication-package/JITLine/JITLine_RQ1-RQ3.ipynb#W5sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m trained_clf_svm, _ \u001b[39m=\u001b[39m train_eval_model(clf_svm, train_feature_res, train_label_res, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenglinwei/Documents/18668/team-2-JITLine-replication-package/JITLine/JITLine_RQ1-RQ3.ipynb#W5sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m                                    test_feature, new_test_label)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenglinwei/Documents/18668/team-2-JITLine-replication-package/JITLine/JITLine_RQ1-RQ3.ipynb#W5sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfinished training the second model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenglinwei/Documents/18668/team-2-JITLine-replication-package/JITLine/JITLine_RQ1-RQ3.ipynb#W5sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m trained_clf_log_reg, _ \u001b[39m=\u001b[39m train_eval_model(clf_log_reg, train_feature_res, train_label_res, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenglinwei/Documents/18668/team-2-JITLine-replication-package/JITLine/JITLine_RQ1-RQ3.ipynb#W5sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m                                     test_feature, new_test_label)\n",
      "File \u001b[0;32m~/Documents/18668/team-2-JITLine-replication-package/JITLine/my_util.py:184\u001b[0m, in \u001b[0;36mtrain_eval_model\u001b[0;34m(clf, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m    181\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    182\u001b[0m clf\u001b[39m.\u001b[39mfit(x_train,y_train)\n\u001b[0;32m--> 184\u001b[0m prob \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39;49mpredict_proba(x_test)[:,\u001b[39m1\u001b[39m]\n\u001b[1;32m    186\u001b[0m pred \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(x_test)\n\u001b[1;32m    188\u001b[0m pred_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/metaestimators.py:127\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__\u001b[0;34m(self, obj, owner)\u001b[0m\n\u001b[1;32m    121\u001b[0m attr_err \u001b[39m=\u001b[39m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(owner\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattribute_name)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m )\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[39m# delegate only on instances, not the classes.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[39m# this is to allow access to the docstrings.\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck(obj):\n\u001b[1;32m    128\u001b[0m         \u001b[39mraise\u001b[39;00m attr_err\n\u001b[1;32m    129\u001b[0m     out \u001b[39m=\u001b[39m MethodType(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn, obj)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/svm/_base.py:819\u001b[0m, in \u001b[0;36mBaseSVC._check_proba\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_proba\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    818\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobability:\n\u001b[0;32m--> 819\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m    820\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mpredict_proba is not available when  probability=False\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    821\u001b[0m         )\n\u001b[1;32m    822\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mc_svc\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnu_svc\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    823\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mpredict_proba only implemented for SVC and NuSVC\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"
     ]
    }
   ],
   "source": [
    "run_experiment('openstack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('The best k_neighbors of Openstack:', openstack_k_of_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiment('qt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('The best k_neighbors of Qt:', qt_k_of_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ1-RQ2 result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_data_dir = './data/'\n",
    "\n",
    "def get_recall_at_k_percent_effort(percent_effort, result_df_arg, real_buggy_commits):\n",
    "    cum_LOC_k_percent = (percent_effort/100)*result_df_arg.iloc[-1]['cum_LOC']\n",
    "    buggy_line_k_percent =  result_df_arg[result_df_arg['cum_LOC'] <= cum_LOC_k_percent]\n",
    "    buggy_commit = buggy_line_k_percent[buggy_line_k_percent['label']==1]\n",
    "    recall_k_percent_effort = len(buggy_commit)/float(len(real_buggy_commits))\n",
    "    \n",
    "    return recall_k_percent_effort\n",
    "\n",
    "def eval_metrics(result_df):\n",
    "    \n",
    "    pred = result_df['defective_commit_pred']\n",
    "    y_test = result_df['label']\n",
    "\n",
    "    ACC = accuracy_score(y_test, pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_test,pred,average='binary') # at threshold = 0.5\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, pred, labels=[0, 1]).ravel()\n",
    "#     rec = tp/(tp+fn)\n",
    "    \n",
    "    FAR = fp/(fp+tn) # false alarm rate\n",
    "    dist_heaven = math.sqrt((pow(1-rec,2)+pow(0-FAR,2))/2.0) # distance to heaven\n",
    "    \n",
    "    AUC = roc_auc_score(y_test, result_df['defective_commit_prob'])\n",
    "\n",
    "    result_df['defect_density'] = result_df['defective_commit_prob']/result_df['LOC'] # predicted defect density\n",
    "    result_df['actual_defect_density'] = result_df['label']/result_df['LOC'] #defect density\n",
    "\n",
    "    result_df = result_df.sort_values(by='defect_density',ascending=False)\n",
    "    actual_result_df = result_df.sort_values(by='actual_defect_density',ascending=False)\n",
    "    actual_worst_result_df = result_df.sort_values(by='actual_defect_density',ascending=True)\n",
    "\n",
    "    result_df['cum_LOC'] = result_df['LOC'].cumsum()\n",
    "    actual_result_df['cum_LOC'] = actual_result_df['LOC'].cumsum()\n",
    "    actual_worst_result_df['cum_LOC'] = actual_worst_result_df['LOC'].cumsum()\n",
    "\n",
    "    real_buggy_commits = result_df[result_df['label'] == 1]\n",
    "\n",
    "    label_list = list(result_df['label'])\n",
    "\n",
    "    all_rows = len(label_list)\n",
    "\n",
    "    # find Recall@20%Effort\n",
    "    cum_LOC_20_percent = 0.2*result_df.iloc[-1]['cum_LOC']\n",
    "    buggy_line_20_percent = result_df[result_df['cum_LOC'] <= cum_LOC_20_percent]\n",
    "    buggy_commit = buggy_line_20_percent[buggy_line_20_percent['label']==1]\n",
    "    recall_20_percent_effort = len(buggy_commit)/float(len(real_buggy_commits))\n",
    "\n",
    "    # find Effort@20%Recall\n",
    "    buggy_20_percent = real_buggy_commits.head(math.ceil(0.2 * len(real_buggy_commits)))\n",
    "    buggy_20_percent_LOC = buggy_20_percent.iloc[-1]['cum_LOC']\n",
    "    effort_at_20_percent_LOC_recall = int(buggy_20_percent_LOC) / float(result_df.iloc[-1]['cum_LOC'])\n",
    "    \n",
    "    # find P_opt\n",
    "    percent_effort_list = []\n",
    "    predicted_recall_at_percent_effort_list = []\n",
    "    actual_recall_at_percent_effort_list = []\n",
    "    actual_worst_recall_at_percent_effort_list = []\n",
    "    \n",
    "    for percent_effort in np.arange(10,101,10):\n",
    "        predicted_recall_k_percent_effort = get_recall_at_k_percent_effort(percent_effort, result_df, real_buggy_commits)\n",
    "        actual_recall_k_percent_effort = get_recall_at_k_percent_effort(percent_effort, actual_result_df, real_buggy_commits)\n",
    "        actual_worst_recall_k_percent_effort = get_recall_at_k_percent_effort(percent_effort, actual_worst_result_df, real_buggy_commits)\n",
    "        \n",
    "        percent_effort_list.append(percent_effort/100)\n",
    "        \n",
    "        predicted_recall_at_percent_effort_list.append(predicted_recall_k_percent_effort)\n",
    "        actual_recall_at_percent_effort_list.append(actual_recall_k_percent_effort)\n",
    "        actual_worst_recall_at_percent_effort_list.append(actual_worst_recall_k_percent_effort)\n",
    "\n",
    "    p_opt = 1 - ((auc(percent_effort_list, actual_recall_at_percent_effort_list) - \n",
    "                 auc(percent_effort_list, predicted_recall_at_percent_effort_list)) /\n",
    "                (auc(percent_effort_list, actual_recall_at_percent_effort_list) -\n",
    "                auc(percent_effort_list, actual_worst_recall_at_percent_effort_list)))\n",
    "    \n",
    "    return ACC, prec, rec, f1, AUC, FAR, dist_heaven, recall_20_percent_effort, effort_at_20_percent_LOC_recall, p_opt\n",
    "\n",
    "def eval_result(proj_name,sampling_method = 'DE_SMOTE_min_df_3'):\n",
    "    \n",
    "    RF_result = pd.read_csv(RF_data_dir+proj_name+'_RF_'+sampling_method+'_prediction_result.csv')\n",
    "    \n",
    "    RF_result.columns = ['Unnamed', 'defective_commit_prob','defective_commit_pred','label','test_commit'] # for new result\n",
    "\n",
    "    test_code, test_commit, test_label = prepare_data(proj_name, mode='test',\n",
    "                                                              remove_python_common_tokens=remove_python_common_tokens)\n",
    "\n",
    "    # get LOC of each commit\n",
    "    RF_LOC = [len(code.splitlines()) for code in test_code]\n",
    "    RF_df = pd.DataFrame()\n",
    "    RF_df['commit_id'] = test_commit\n",
    "    RF_df['LOC'] = RF_LOC\n",
    "\n",
    "    RF_result = pd.merge(RF_df, RF_result,how='inner',left_on = 'commit_id', right_on='test_commit')\n",
    "    acc, prec, rec, f1, auc, FAR, dist_heaven, recall_20_percent_effort, effort_at_20_percent_LOC_recall,p_opt = eval_metrics(RF_result)\n",
    "    \n",
    "    \n",
    "    print('Accuracy: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, F1: {:.2f}, AUC: {:.2f}, FAR: {:.2f}, d2h: {:.2f}, PCI@20%LOC: {:.2f}, Effort@20%Recall: {:.2f}, POpt: {:.2f}'.format(acc, prec, rec, f1, auc, FAR, dist_heaven, recall_20_percent_effort, effort_at_20_percent_LOC_recall,p_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1331/1331 [00:02<00:00, 588.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88, Precision: 0.51, Recall: 0.29, F1: 0.37, AUC: 0.83, FAR: 0.04, d2h: 0.50, PCI@20%LOC: 0.58, Effort@20%Recall: 0.04, POpt: 0.83\n"
     ]
    }
   ],
   "source": [
    "eval_result('openstack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_result('qt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3 result\n",
    "\n",
    "note: the_best_k_neighbors is obtained from model training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_train_time(cur_proj, the_best_k_neighbors):\n",
    "    data_path = './data/'\n",
    "    model_path = './final_model/'\n",
    "        \n",
    "    train_code, train_commit, train_label = prepare_data(cur_proj, mode='train',\n",
    "                                                                  remove_python_common_tokens=remove_python_common_tokens)\n",
    "\n",
    "    commit_metrics = load_change_metrics_df(cur_proj)\n",
    "    train_commit_metrics = commit_metrics[commit_metrics['commit_id'].isin(train_commit)]\n",
    "    \n",
    "    count_vect = CountVectorizer(min_df=3, ngram_range=(1,1))\n",
    "    count_vect.fit(train_code)\n",
    "    \n",
    "    print('fit countvectorizer finished')\n",
    "    \n",
    "    train_feature, train_commit_id, new_train_label = get_combined_df(train_code, train_commit, train_label, train_commit_metrics,count_vect)\n",
    "\n",
    "    percent_80 = int(len(new_train_label)*0.8)\n",
    "    \n",
    "    final_train_feature = train_feature[:percent_80]\n",
    "    final_train_commit_id = train_commit_id[:percent_80]\n",
    "    final_new_train_label = new_train_label[:percent_80]\n",
    "    \n",
    "    smote = SMOTE(random_state=42, n_jobs=1, k_neighbors=the_best_k_neighbors)\n",
    "    \n",
    "    train_feature_res, train_label_res = smote.fit_resample(final_train_feature, final_new_train_label)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)\n",
    "    clf_name = 'RF'\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    clf.fit(train_feature_res, train_label_res)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    train_time = end-start\n",
    "    print('train time of {} is {:.3f} secs'.format(cur_proj,train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit countvectorizer finished\n",
      "train time of openstack is 35.702 secs\n"
     ]
    }
   ],
   "source": [
    "check_train_time('openstack', 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit countvectorizer finished\n",
      "train time of qt is 174.545 secs\n"
     ]
    }
   ],
   "source": [
    "check_train_time('qt', 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
